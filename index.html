<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="CoReS: Orchestrating the Dance of Reasoning and Segmentation">
  <meta name="keywords" content="CoReS">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>CoReS: Orchestrating the Dance of Reasoning and Segmentation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/ms_icon.png">

  <style>  
    table {  
      font-family: arial, sans-serif;  
      border-collapse: collapse;  
      width: 100%;  
    }  
      
    td, th {  
      border: 2px solid #F1F4F5;  
      text-align: left;  
      padding: 8px;  
    }  
    tr:nth-child(3n - 1) {  
      background-color: #F1F4F5;  
    }  

    tr:nth-child(3n) {  
      border: 2px solid #FFFFFF;
    }  
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">CoReS: Orchestrating the Dance of Reasoning and Segmentation </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=gSI_eiIAAAAJ">Xiaoyi Bao</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=IK9DwSIAAAAJ">Siyang Sun</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=dNhzCu4AAAAJ">Shuailei Ma</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=hMDQifQAAAAJ">Kecheng Zheng</a><sup>4</sup>,</span>
            <span class="author-block">
              <a >Yuxin Guo</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=C3BbrU4AAAAJ">Guosheng Zhao</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?hl=zh-CN&user=-hFpScAAAAAJ">Yun Zheng</a><sup>2</sup>,</span>
            <span class="author-block">
              <a >Xingang Wang</a><sup>1</sup></span>
            </span><br>

          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">CASIA<sup>1</sup>,</span>&nbsp;
            <span class="author-block">Alibaba Group<sup>2</sup>,</span>&nbsp;
            <span class="author-block">Northeastern University<sup>3</sup>,</span>&nbsp;
            <span class="author-block">Ant Group<sup>4</sup></span>&nbsp;
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2404.05673.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/baoxiaoyi/CoReS"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img id="teaser" autoplay muted loop playsinline height="100%" src="./static/images/comparison.png" style="width:100%;height:100%;">
      <p  style="font-size: 16px;"> 
        Comparison between our CoReS and LISA. UP: the process of LISA, DOWN: the diagram of CoReS. Given textual and visual inputs, LISA directly uses the [SEG] token output by MLLM to generate a mask. On the contrary, our CoReS involves breaking down the task of "finding the part that gives dogs keen sense of smell" into a logical chain such as "first find the front part of the dog's face, then focus on this specific area, search for the nose of the dog." It can be observed that LISA incorrectly segments the dog's eyes, which are similarly round, dark, and important in sensory perception. In contrast, through in-context input and dual-chain structure, CoReS achieves the segmentation of the nose of the dog correctly. 
      </p>
    </div>
    <div class="hero-body">
      <img id="teaser" autoplay muted loop playsinline height="100%" src="./static/images/sample.png" style="width:100%;height:100%;">
    <p  style="font-size: 16px;"> 
      Visual comparison of CoReS and LISA.
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The reasoning segmentation task, which demands a nuanced comprehension of intricate queries to accurately pinpoint object regions, is attracting increasing attention. 
	    However, Multi-modal Large Language Models (MLLM) often find it difficult to accurately localize the objects described in complex reasoning contexts. 
	    We believe that the act of reasoning segmentation should mirror the cognitive stages of human visual search, where each step is a progressive refinement of thought toward the final object. 
	    Thus we introduce the Chains of Reasoning and Segmenting (CoReS) and find this top-down visual hierarchy indeed enhances the visual search process. Specifically, we propose a dual-chain structure that generates multi-modal, chain-like outputs to aid the segmentation process. 
	    Furthermore, to steer the MLLM's outputs into this intended hierarchy, we incorporate in-context inputs as guidance. Extensive experiments demonstrate the superior performance of our CoReS, which surpasses the state-of-the-art method by 6.5% on the ReasonSeg dataset. 
	  </p>
        </div> 
      </div>
    </div>
    <!--/ Abstract. -->


  </div>
</section>

	
<section class="section" id="Method">
  <div class="container is-max-desktop content">
    <h2 class="title">Method</h2>
    <section class="hero method">
      <div class="container is-max-desktop">
        <div class="hero-body">
          <img id="method" autoplay muted loop playsinline height="100%" src="./static/images/method.png" style="width:100%;height:100%;">
          <p>
            Overall architecture of CoReS. The input of MLLM consists of the user input in gray and the extra in-context input in orange, which consists of question-answer examples unrelated to the user query. MLLM generates output at the logical level of chain-of-reasoning, where the token embeddings of [LOC] and [SEG] serve as prompt inputs for different positions of the segmentation chain, guiding the chain to generate segmentation results progressively. For conciseness, the diagram excludes the feature extraction from the image using the vision backbone and its input to the mask decoder.
          </p>
        </div>
      </div>
    </section>
  </div>
</section>


<section class="section" id="Results">
  <div class="container is-max-desktop content">
    <h2 class="title">Results</h2>
    <section class="hero method">
    <div class="container is-max-desktop">
    <div class="hero-body">  

<div class="container is-max-desktop">
    <div style="text-align: center;">
      <img id="table1" autoplay muted loop playsinline height="100%" src="./static/images/table1.png" style="width:100%;height:100%;">
    </div>
  </div>
  <br>
	<p style="margin-bottom: 30px;"></p>
	<h4 class="title">Qualitative interpretation of the advantages of the multi-modal chain-of-thought over LISA. From left to right are the input image, LISA result, CoReS first logic layer segmentation result, CoReS final result, and ground truth mask.</h4>
  <div class="container is-max-desktop">
    <div style="text-align: center;">
      <img id="table1" autoplay muted loop playsinline height="100%" src="./static/images/5picresult.png" style="width:100%;height:100%;">
    </div>
  </div>
  <br>
	<p style="margin-bottom: 30px;"></p>
  <h4 class="title">More visual comparisons of CoReS and LISA.</h4>
  <div class="container is-max-desktop">
    <div style="text-align: center;">
      <img id="table1" autoplay muted loop playsinline height="100%" src="./static/images/append1.png" style="width:100%;height:100%;">
    </div>
    <div style="text-align: center;">
      <img id="table1" autoplay muted loop playsinline height="100%" src="./static/images/append2.png" style="width:100%;height:100%;">
    </div>
  </div>
  <br>
	<p style="margin-bottom: 30px;"></p>


  </div></div></section>
  </div>
</section>

	
	
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <p> If you use our work in your research, please cite: </p>
    <pre><code>@article{bao2024cores,
  title={CoReS: Orchestrating the Dance of Reasoning and Segmentation},
  author={Bao, Xiaoyi and Sun, Siyang and Ma, Shuailei and Zheng, Kecheng and Guo, Yuxin and Zhao, Guosheng and Zheng, Yun and Wang, Xingang},
  journal={arXiv preprint arXiv:2404.05673},
  year={2024}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link" href="https://github.com/baoxiaoyi/CoReS" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website adapted from the following <a href="https://github.com/nerfies/nerfies.github.io">template</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
